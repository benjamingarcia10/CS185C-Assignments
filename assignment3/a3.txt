1) For each of the 100+100 files you produced in assignment #2; normalize the helpfulness by dividing the helpful_votes by total_votes (just in case your file is missing a total_votes column you can skip this step); then find the median of normalized helpfulness scores in each file (google how to use awk for this).

// Normalize customers helpfulness score
for file in `ls ~/CS185C-Assignments/assignment2/CUSTOMERS` ; do customerID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d;`; if [ ! -z "$customerID" ]; then grep $customerID ~/amazon_reviews_us_Books_v1_02.tsv | cut -f 8-10 | awk -F '\t' '{if ($3 != 0) print $2, $1, $3, $2/$3; else print $2, $1, $3, $3 }' OFS='\t' > ~/a2/CUSTOMERS/$customerID.TOTAL.txt; echo "customerID: $customerID, count: $count"; ((count ++)); fi; done

// Normalize products helpfulness score
for file in `ls ~/CS185C-Assignments/assignment2/PRODUCTS` ; do productID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d;`; if [ ! -z "$productID" ]; then grep $productID ~/amazon_reviews_us_Books_v1_02.tsv | cut -f 8-10 | awk -F '\t' '{if ($3 != 0) print $2, $1, $3, $2/$3; else print $2, $1, $3, $3 }' OFS='\t' > ~/a2/PRODUCTS/$productID.TOTAL.txt; echo "productID: $productID, count: $count"; ((count ++)); fi; done

// Get customer medians
for file in `ls ~/CS185C-Assignments/assignment2/CUSTOMERS` ; do customerID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d'`; if [ ! -z "$customerID" ]; then median=`cut -f 4 ~/CS185C-Assignments/assignment2/CUSTOMERS/$customerID.TOTAL.txt | sort -n | awk ' { a[i++]=$1; } END { x=int((i+1)/2); if (x < (i+1)/2) print (a[x-1]+a[x])/2; else print a[x-1]; }'`; echo -e "$customerID\t$median" >> customer_median.txt; fi; done

// Get product medians
for file in `ls ~/CS185C-Assignments/assignment2/PRODUCTS` ; do productID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d'`; if [ ! -z "$productID" ]; then median=`cut -f 4 ~/CS185C-Assignments/assignment2/PRODUCTS/$productID.TOTAL.txt | sort -n | awk ' { a[i++]=$1; } END { x=int((i+1)/2); if (x < (i+1)/2) print (a[x-1]+a[x])/2; else print a[x-1]; }'`; echo -e "$productID\t$median" >> product_median.txt; fi; done





2) Then convert the helpfulness column in each file to binary 0 or 1 values: a helpfulness score becomes 0 if it is below the median, and 1 if it is over the median (for that particular file).

// Compare medians for customers and convert helpfulness column to binary
for file in `ls ~/CS185C-Assignments/assignment2/CUSTOMERS` ; do customerID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d'`; if [ ! -z "$customerID" ]; then median=`grep $customerID ~/CS185C-Assignments/assignment3/customer_median.txt | cut -f 2`; awk -F '\t' -v median="$median" '{if ($4 < median) print 0, $2; else print 1, $2}' OFS='\t' ~/CS185C-Assignments/assignment2/CUSTOMERS/$customerID.TOTAL.txt > ~/CS185C-Assignments/assignment2/CUSTOMERS/$customerID.BINARY.txt; fi; done

// Compare medians for products and convert helpfulness column to binary
for file in `ls ~/CS185C-Assignments/assignment2/PRODUCTS` ; do productID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d'`; if [ ! -z "$productID" ]; then median=`grep $productID ~/CS185C-Assignments/assignment3/product_median.txt | cut -f 2` ;awk -F '\t' -v median="$median" '{if ($4 < median) print 0, $2; else print 1, $2}' OFS='\t' ~/CS185C-Assignments/assignment2/PRODUCTS/$productID.TOTAL.txt > ~/CS185C-Assignments/assignment2/PRODUCTS/$productID.BINARY.txt; fi; done





4) Recompute the correlation scores between rating stars and helpfulness, as you did in a2.

cd ~/datamash-1.3

// Recompute customer correlation
for file in `ls ~/CS185C-Assignments/assignment2/CUSTOMERS` ; do customerID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d'`; if [ ! -z "$customerID" ]; then correlation=`./datamash -W ppearson 1:2 < ~/CS185C-Assignments/assignment2/CUSTOMERS/$customerID.BINARY.txt`; echo -e "$customerID\t$correlation" >> ~/CS185C-Assignments/assignment3/customer_correlation.txt; fi; done

// Recompute product correlation
for file in `ls ~/CS185C-Assignments/assignment2/PRODUCTS` ; do productID=`echo $file | sed -e 's/\.txt//;/.TOTAL/d;/.BINARY/d'`; if [ ! -z "$productID" ]; then correlation=`./datamash -W ppearson 1:2 < ~/CS185C-Assignments/assignment2/PRODUCTS/$productID.BINARY.txt`; echo -e "$productID\t$correlation" >> ~/CS185C-Assignments/assignment3/product_correlation.txt; fi; done





5) Plot the review rating stars and helpfulness scores for the top customer (the customer with the highest correlation score) in a line plot. You can find the gnuplot commands here:
sort -k 2 -r customer_correlation.txt > customer_correlation.sorted.txt

// Top scores from output
53071109	0.57689851289561
51065232	0.52308126332041
52173832	0.51917603969455
52254603	0.44273868279524
52706646	0.39044971527043





6) Plot the review rating stars and helpfulness scores for the top product (the product with the highest correlation score) in another line plot.
sort -k 2 -r product_correlation.txt > product_correlation.sorted.txt

// Top scores from output
0060875410	0.88627167310378
0060763957	0.74303694227162
0895260174	0.66760818665021
0066214130	0.65060307273362
0525947647	0.61097378252701





7) Do you think the correlation scores are more meaningful now than before in a2? Explain in a few sentences what you think.
Yes, I think it is more meaningful now since the scores have been normalized and is not easily skewed by a high helpfulness score but only a small percentage of voters said it was helpful.

8) In a previous worksheet #7 you extracted words (tokens), html tags, and removed a few stop words for one product reviews file. You will do a similar thing. First put in a file the review_body and helpfulness scores for one product (you want to use a product file that has plenty of reviews, such that you have enough data to work with; you can also use the same product as for worksheet #7 if you prefer).

grep 0060193395 ~/amazon_reviews_us_Books_v1_02.tsv | cut -f 9,14 > 0060193395.REVIEWBODY.txt





9) Remove the stop words and html tags from the review_body. Then also remove all small 1-2 character words (any words with one character or two characters) from the review body.

sed -i 's/<[^/]*\/>//g' 0060193395.REVIEWBODY.txt
sed -i 's/\b[a-zA-Z]\{1,2\}\b//g' 0060193395.REVIEWBODY.txt
sed -i -e 's/[\.,;]//g' -e 's/\band\b//g' -e 's/\bor\b//g' -e 's/\bif\b//g' -e 's/\bin\b//g' -e 's/\bit\b//g' 0060193395.REVIEWBODY.txt





10) Which 10 words appear most frequently in the helpfulness=1 category? Which 10 words appear most frequently in the helpfulness=0 category?

// Most frequent words in helpfulness=1 category
awk -F '\t' '{if($1==1) print $2}' OFS=' ' 0060193395.REVIEWBODY.txt | tr -s ' ' | tr '[:space:]' '[\n*]' | sort | uniq -c | sort -k1 -r | head -n 15
    274 the
    126 you
    100 book
     97 for
     78 have
     66 that
     61 this
     54 '
     50 are
     46 program
     45 not
     41 but
     37 with
     33 your
     32 his

// Most frequent words in helpfulness=0 category
awk -F '\t' '{if($1==0) print $2}' OFS=' ' 0060193395.REVIEWBODY.txt | tr -s ' ' | tr '[:space:]' '[\n*]' | sort | uniq -c | sort -k1 -r | head -n 15
    323 the
    160 book
    159 you
    128 that
    124 for
    102 this
     76 have
     55 your
     55 Bill
     52 '
     49 program
     46 will
     45 his
     45 but
     43 read
